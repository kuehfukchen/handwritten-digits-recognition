{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 12)        108       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 12)       36        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 12)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 24)        10368     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 24)       72        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 24)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 32)          27648     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 32)         96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               313600    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 200)              600       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354,538\n",
      "Trainable params: 354,002\n",
      "Non-trainable params: 536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False, padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(kernel_size=6, filters=24, use_bias=False, padding='same', strides=2),\n",
    "        tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(kernel_size=6, filters=32, use_bias=False, padding='same', strides=2),\n",
    "        tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        tf.keras.layers.Dense(200, use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        \n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "model = create_model()\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(992, 784), (992,), (111, 784), (111,)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,y = get_data()\n",
    "# X = X.reshape(-1, 28*28)\n",
    "train_digit,test_digit, train_label, test_label = train_test_split(X,y, test_size=0.1, random_state=42)\n",
    "[tmp.shape for tmp in [train_digit, train_label,test_digit, test_label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch:  99\n",
      "Epoch 1/10\n",
      "99/99 [==============================] - 4s 24ms/step - loss: 0.8594 - accuracy: 0.7407 - val_loss: 0.6708 - val_accuracy: 0.8018\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2558 - accuracy: 0.9333 - val_loss: 0.3987 - val_accuracy: 0.8829\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1837 - accuracy: 0.9611 - val_loss: 0.2297 - val_accuracy: 0.9279\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1380 - accuracy: 0.9667 - val_loss: 0.1769 - val_accuracy: 0.9279\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.0939 - accuracy: 0.9796 - val_loss: 0.1509 - val_accuracy: 0.9369\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.0844 - accuracy: 0.9815 - val_loss: 0.2305 - val_accuracy: 0.9369\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.0647 - accuracy: 0.9898 - val_loss: 0.1776 - val_accuracy: 0.9279\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.0719 - accuracy: 0.9852 - val_loss: 0.1179 - val_accuracy: 0.9369\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0330 - accuracy: 0.9963 - val_loss: 0.1193 - val_accuracy: 0.9730\n",
      "Epoch 10/10\n",
      "17/99 [====>.........................] - ETA: 1s - loss: 0.0370 - accuracy: 0.9947WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 990 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 990 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 19ms/step - loss: 0.0388 - accuracy: 0.9950 - val_loss: 0.1363 - val_accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "train_label = tf.one_hot(train_label,10)\n",
    "test_label = tf.one_hot(test_label,10)\n",
    "\n",
    "TRAIN_SIZE = len(train_digit)\n",
    "EPOCHS = 10\n",
    "steps_per_epoch = TRAIN_SIZE//EPOCHS\n",
    "BATCH_SIZE  = TRAIN_SIZE//steps_per_epoch\n",
    "\n",
    "print(\"Steps per epoch: \", steps_per_epoch)\n",
    "history = model.fit(x = train_digit, y = train_label , steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                    validation_data=(test_digit, test_label), validation_steps=1)\n",
    "model.save('saved_model/local_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(60000, 784), TensorShape([60000, 10]), (10000, 784), TensorShape([10000, 10])]\n",
      "Steps per epoch:  468\n",
      "Epoch 1/10\n",
      "468/468 [==============================] - 59s 123ms/step - loss: 0.1438 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 0.9809\n",
      "Epoch 2/10\n",
      "468/468 [==============================] - 54s 115ms/step - loss: 0.0424 - accuracy: 0.9875 - val_loss: 0.1797 - val_accuracy: 0.9464\n",
      "Epoch 3/10\n",
      "468/468 [==============================] - 52s 110ms/step - loss: 0.0295 - accuracy: 0.9915 - val_loss: 0.0363 - val_accuracy: 0.9883\n",
      "Epoch 4/10\n",
      "468/468 [==============================] - 54s 115ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 0.0257 - val_accuracy: 0.9908\n",
      "Epoch 5/10\n",
      "468/468 [==============================] - 54s 116ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.0319 - val_accuracy: 0.9890\n",
      "Epoch 6/10\n",
      "468/468 [==============================] - 54s 116ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0441 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "468/468 [==============================] - 54s 116ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.0272 - val_accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "468/468 [==============================] - 54s 115ms/step - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.0454 - val_accuracy: 0.9867\n",
      "Epoch 9/10\n",
      "468/468 [==============================] - 54s 115ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0393 - val_accuracy: 0.9884\n",
      "Epoch 10/10\n",
      "447/468 [===========================>..] - ETA: 2s - loss: 0.0074 - accuracy: 0.9979WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4680 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4680 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 52s 110ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0253 - val_accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "RUN_MNIST = False\n",
    "if RUN_MNIST:\n",
    "    model = create_model()\n",
    "    train_digit, train_label = mnist.extract_train()\n",
    "    test_digit, test_label = mnist.extract_test()\n",
    "    train_digit = train_digit.reshape(-1,28*28)\n",
    "    test_digit = test_digit.reshape(-1,28*28)\n",
    "\n",
    "    train_label = tf.one_hot(train_label, 10)\n",
    "    test_label = tf.one_hot(test_label, 10)\n",
    "    print([tmp.shape for tmp in [train_digit, train_label, test_digit, test_label]])\n",
    "\n",
    "    TRAIN_SIZE = len(train_digit)\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 10\n",
    "    steps_per_epoch = TRAIN_SIZE//BATCH_SIZE  # 60,000 items in this dataset\n",
    "\n",
    "\n",
    "    print(\"Steps per epoch: \", steps_per_epoch)\n",
    "    history = model.fit(x = train_digit, y = train_label , steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=(test_digit, test_label), validation_steps=1)\n",
    "    model.save('saved_model/googler_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.21396192203082"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits, labels = get_data()\n",
    "digits = digits.reshape(-1,28*28)\n",
    "\n",
    "pred = model.predict(digits)\n",
    "pred = [np.argmax(p) for p in pred]\n",
    "pred = np.array(pred)\n",
    "\n",
    "res = pred == labels\n",
    "acc = res[res].sum() / res.shape[0] * 100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.13"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits, labels = mnist.extract_test()\n",
    "# digits, labels = get_data()\n",
    "digits = digits.reshape(-1,28*28)\n",
    "\n",
    "pred = model.predict(digits)\n",
    "pred = [np.argmax(p) for p in pred]\n",
    "pred = np.array(pred)\n",
    "\n",
    "res = pred == labels\n",
    "acc = res[res].sum() / res.shape[0] * 100\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2324a66782fee6ab9f0bdb3c9e79ee636ed86487484f245c4444858429ce7730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
