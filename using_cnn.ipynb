{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 12)        108       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 12)       36        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 12)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 24)        10368     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 24)       72        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 24)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 32)          27648     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 32)         96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               313600    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 200)              600       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354,538\n",
      "Trainable params: 354,002\n",
      "Non-trainable params: 536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(\n",
    "  [\n",
    "      tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1)),\n",
    "      \n",
    "      tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False, padding='same'),\n",
    "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      \n",
    "      tf.keras.layers.Conv2D(kernel_size=6, filters=24, use_bias=False, padding='same', strides=2),\n",
    "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      \n",
    "      tf.keras.layers.Conv2D(kernel_size=6, filters=32, use_bias=False, padding='same', strides=2),\n",
    "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      \n",
    "      tf.keras.layers.Flatten(),\n",
    "      \n",
    "      tf.keras.layers.Dense(200, use_bias=False),\n",
    "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
    "      tf.keras.layers.Activation('relu'),\n",
    "      \n",
    "      tf.keras.layers.Dropout(0.3),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_digit, train_label = mnist.extract_train()\n",
    "test_digit, test_label = mnist.extract_test()\n",
    "\n",
    "train_digit = train_digit.reshape(-1,28*28)\n",
    "test_digit = test_digit.reshape(-1,28*28)\n",
    "\n",
    "train_label = tf.one_hot(train_label, 10)\n",
    "test_label = tf.one_hot(test_label, 10)\n",
    "\n",
    "print([tmp.shape for tmp in [train_digit, train_label, test_digit, test_label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch:  468\n",
      "Epoch 1/10\n",
      "468/468 [==============================] - 60s 124ms/step - loss: 0.1394 - accuracy: 0.9594 - val_loss: 0.0578 - val_accuracy: 0.9814\n",
      "Epoch 2/10\n",
      "468/468 [==============================] - 55s 118ms/step - loss: 0.0436 - accuracy: 0.9870 - val_loss: 0.0673 - val_accuracy: 0.9798\n",
      "Epoch 3/10\n",
      "468/468 [==============================] - 54s 116ms/step - loss: 0.0307 - accuracy: 0.9907 - val_loss: 0.0300 - val_accuracy: 0.9904\n",
      "Epoch 4/10\n",
      "468/468 [==============================] - 54s 116ms/step - loss: 0.0241 - accuracy: 0.9927 - val_loss: 0.0326 - val_accuracy: 0.9891\n",
      "Epoch 5/10\n",
      "468/468 [==============================] - 63s 135ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0279 - val_accuracy: 0.9898\n",
      "Epoch 6/10\n",
      "468/468 [==============================] - 60s 128ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.0273 - val_accuracy: 0.9914\n",
      "Epoch 7/10\n",
      "468/468 [==============================] - 77s 165ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.0899 - val_accuracy: 0.9734\n",
      "Epoch 8/10\n",
      "468/468 [==============================] - 69s 148ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0270 - val_accuracy: 0.9912\n",
      "Epoch 9/10\n",
      "468/468 [==============================] - 77s 165ms/step - loss: 0.0085 - accuracy: 0.9973 - val_loss: 0.0394 - val_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "447/468 [===========================>..] - ETA: 3s - loss: 0.0093 - accuracy: 0.9968WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4680 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 4680 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 68s 145ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.0239 - val_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
    "\n",
    "\n",
    "print(\"Steps per epoch: \", steps_per_epoch)\n",
    "history = model.fit(x = train_digit, y = train_label , steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
    "                    validation_data=(test_digit, test_label), validation_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/local_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/local_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/local_model')\n",
    "model = tf.keras.models.load_model('saved_model/local_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.22999999999999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits, labels = mnist.extract_test()\n",
    "digits = digits.reshape(-1,28*28)\n",
    "\n",
    "pred = model.predict(digits)\n",
    "pred = [np.argmax(p) for p in pred]\n",
    "pred = np.array(pred)\n",
    "\n",
    "res = pred == labels\n",
    "acc = res[res].sum() / res.shape[0] * 100\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2324a66782fee6ab9f0bdb3c9e79ee636ed86487484f245c4444858429ce7730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
